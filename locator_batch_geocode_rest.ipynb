{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Field Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "Dictionary template for ArcGIS Multiline address format. Your input CSV column headers should conform to the following:\n",
    "    arcgis_address_format = {\n",
    "        \"Address\": \"\",\n",
    "        \"Neighborhood\": \"\",\n",
    "        \"City\": \"\",\n",
    "        \"Subregion\": \"\",  # Typically county or equivalent\n",
    "        \"Region\": \"\",  # Typically state or equivalent\n",
    "        \"Postal\": \"\",\n",
    "        \"CountryCode\": \"\"\n",
    "    }\n",
    "    ````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import csv\n",
    "import requests\n",
    "import json\n",
    "import urllib.parse\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Parameters\n",
    "\n",
    "Use the following parameters to control the geocoding job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "csv_file_path = '/Users/maples/GitHub/Locator-Scripts/Data/oneMillionAddresses.csv' # THe path to your input csv. \n",
    "\n",
    "output_csv_path = '/Users/maples/GitHub/Locator-Scripts/Data/geocoded_records01.csv' # Rename your output file here, with the full path to the file\n",
    "\n",
    "arcgis_service_url = 'https://locator.stanford.edu/arcgis/rest/services/geocode/NorthAmerica/GeocodeServer/geocodeAddresses'\n",
    "\n",
    "jobSize = 'all' # this parameter controls the first N number of records that will be processed when the tool is run. FOr all records, replace the integer value with 'all' (numeric input must be an integer, without quotes)\n",
    "\n",
    "chunkSize = 20 # This parameter controls how many address records are submitted for processing, at a time. It is set to 20, which is fairly optimal, but you can experiment with other values. Much more than 20 will result in URI too long errors, when submitting REST Get Requests\n",
    "\n",
    "outFields = '*' # '*' results in all output fields being included in the output csv. 'none' results in minimal returned output fields (lat & long).\n",
    "\n",
    "printJob='no' # This parameter indicates whether you want the Get requests written to the console for debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare and submit GET Requests from CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `geocode_addresses` function in the provided Python code is designed to process a CSV file and geocode addresses using the ArcGIS Server GeocodeAddresses service. \n",
    "\n",
    "The function takes six parameters: \n",
    "- `csv_file_path` is the path to the input CSV file that contains the addresses to be geocoded.\n",
    "- `arcgis_service_url` is the URL to the ArcGIS GeocodeAddresses service.\n",
    "- `jobSize` is the total number of addresses to process. It can be 'all' for all addresses or an integer for a specific number of addresses.\n",
    "- `chunkSize` is the number of addresses to include in each API request.\n",
    "- `outFields` is a comma-separated list of fields to include in the output.\n",
    "- `printJob` is a string that, if set to 'yes', will print each GET request URL to the console.\n",
    "\n",
    "The function starts by reading the CSV file and storing the addresses in a list. It then calculates the total number of records and prepares for batch processing by dividing the addresses into chunks of size `chunkSize`.\n",
    "\n",
    "For each batch, the function constructs the 'addresses' parameter for the API request and encodes the parameters into a URL. If `printJob` is 'yes', it prints the request URL. It then sends a GET request to the ArcGIS service.\n",
    "\n",
    "If the response status code is 200, the function processes each location in the response. It ensures that 'attributes' is a dictionary and appends it to the `processed_records` list. If the 'attributes' is not a dictionary, it prints a warning message. If the status code is not 200, it prints an error message.\n",
    "\n",
    "The function also provides progress reporting. It calculates the number of processed and remaining records, the elapsed time, the estimated total time, and the estimated remaining time, and prints these details.\n",
    "\n",
    "Finally, the function determines the fieldnames from the processed records and writes the results to a new CSV file. The fieldnames are determined by iterating over the processed records and updating a set with the keys of each record. The results are written to the CSV file using a `csv.DictWriter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def geocode_addresses(csv_file_path, arcgis_service_url, jobSize, chunkSize, outFields, printJob):\n",
    "    \"\"\"\n",
    "    Processes a CSV file to geocode addresses using the ArcGIS Server GeocodeAddresses service.\n",
    "\n",
    "    Parameters:\n",
    "    csv_file_path (str): Path to the input CSV file.\n",
    "    arcgis_service_url (str): URL to the ArcGIS GeocodeAddresses service.\n",
    "    jobSize (str|int): The total number of addresses to process ('all' for all addresses or an integer).\n",
    "    chunkSize (int): Number of addresses to include in each API request.\n",
    "    outFields (str): Comma-separated list of fields to include in the output.\n",
    "    printJob (str): If 'yes', print each GET request URL to the console.\n",
    "    \"\"\"\n",
    "    # Read the CSV file\n",
    "    with open(csv_file_path, mode='r', newline='', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        addresses = list(reader)[:jobSize if isinstance(jobSize, int) else None]\n",
    "\n",
    "    total_records = len(addresses)\n",
    "    processed_records = []\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Prepare batches of addresses for chunked processing\n",
    "    batches = [addresses[i:i + chunkSize] for i in range(0, len(addresses), chunkSize)]\n",
    "\n",
    "    for batch_index, batch in enumerate(batches):\n",
    "        # Construct the 'addresses' parameter for the API request\n",
    "        records = {\n",
    "            \"records\": [\n",
    "                {\n",
    "                    \"attributes\": {\n",
    "                        \"OBJECTID\": idx + batch_index * chunkSize,\n",
    "                        **{key: record[key] for key in record}\n",
    "                    }\n",
    "                } for idx, record in enumerate(batch)\n",
    "            ]\n",
    "        }\n",
    "        params = {\n",
    "            'addresses': json.dumps(records),\n",
    "            'outFields': outFields,\n",
    "            'f': 'pjson'\n",
    "        }\n",
    "        encoded_params = urllib.parse.urlencode(params, quote_via=urllib.parse.quote)\n",
    "\n",
    "        # Construct the full URL for the GET request\n",
    "        request_url = f\"{arcgis_service_url}?{encoded_params}\"\n",
    "\n",
    "        if printJob.lower() == 'yes':\n",
    "            print(f\"Request URL: {request_url}\")\n",
    "\n",
    "                # Send the GET request\n",
    "        response = requests.get(request_url)\n",
    "        if response.status_code == 200:\n",
    "            # Process each location in the response\n",
    "            for location in response.json().get('locations', []):\n",
    "                # Ensure that 'attributes' is a dictionary\n",
    "                if isinstance(location.get('attributes'), dict):\n",
    "                    processed_records.append(location['attributes'])\n",
    "                else:\n",
    "                    print(f\"Warning: Unexpected data format in response: {location}\")\n",
    "        else:\n",
    "            print(f\"Error processing batch {batch_index}: {response.text}\")\n",
    "\n",
    "\n",
    "        # Progress reporting\n",
    "        processed = batch_index * chunkSize + len(batch)\n",
    "        remaining = total_records - processed\n",
    "        elapsed_time = time.time() - start_time\n",
    "        estimated_total_time = elapsed_time / processed * total_records\n",
    "        estimated_remaining_time = estimated_total_time - elapsed_time\n",
    "        print(f\"Processed {processed}/{total_records} records. Remaining: {remaining}. Estimated time to finish: {time.strftime('%H:%M:%S', time.gmtime(estimated_remaining_time))}\")\n",
    "\n",
    "    # Determine fieldnames from the processed records\n",
    "    fieldnames = set()\n",
    "    for record in processed_records:\n",
    "        fieldnames.update(record.keys())\n",
    "\n",
    "    # Output the results to a new CSV file\n",
    "\n",
    "    with open(output_csv_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for record in processed_records:\n",
    "            writer.writerow(record)\n",
    "\n",
    "# Example usage\n",
    "geocode_addresses(csv_file_path, arcgis_service_url, jobSize, chunkSize, outFields, printJob)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gdal-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
